# -*- coding: utf-8 -*-
"""projeto_seg_policial_colab_entrega2_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1kuOapQabHQ54rLODuutf5HKPJSK_Cg

# Projeto: Previsão de **Risco Operacional** para Segurança Policial
Storytelling do Projeto – Predição de Risco em Ocorrências Policiais

Equipe:
Abraão Saraiva,
Carlos Henrique,
Klara Marinho,
Lucas Eduardo,
Luiz Reis.

1. O Problema

Imagine a seguinte situação: a polícia recebe milhares de ocorrências todos os meses. Algumas são de menor gravidade, mas outras envolvem situações de alto risco, como uso de armas de fogo ou explosivos.
O grande desafio é saber, com antecedência, quais ocorrências merecem maior atenção, para que protocolos de segurança sejam acionados de forma rápida e eficiente.
Esse foi o ponto de partida do projeto: construir um modelo de predição de risco que apoie a tomada de decisão operacional.

Definimos que uma ocorrência seria considerada “alto risco” se envolvesse arma de fogo ou explosivos. Essa foi a nossa variável-alvo.

# Contexto, Problema e Storytelling

**Tema do grupo:** Identificar os **crimes mais perigosos para o policial** e os **locais com maior probabilidade de ocorrência**, a fim de apoiar **planejamento operacional** (alocação de patrulhamento, priorização de rondas e uso de recursos).

- **Pergunta de pesquisa**: Onde/Quando há maior risco operacional para o policial, considerando tipo de crime, local e momento (dia/horário)?  
- **Objetivo (MVP supervisionado)**: Modelar a probabilidade de um **crime perigoso** ocorrer (variável-alvo binária), dado um conjunto de atributos contextuais (tempo, espaço, tipo, etc.).  
- **Hipóteses**:  
  1) Determinadas regiões concentram maior risco por características ambientais/sociais.  
  2) Faixas horárias/dias específicos elevam a chance de crimes perigosos.  
  3) O tipo de ocorrência está associado a níveis distintos de risco para o policial.

> **Como ler este notebook**: primeiro validamos qualidade do dado (EDA), depois montamos um pipeline reprodutível de pré-processamento, testamos **baseline + 2 modelos**, avaliamos com métricas apropriadas e, por fim, interpretamos os resultados para orientar decisões.

## Pipeline de Pré-processamento (guia rápido)

Boas práticas exigidas na entrega:
- **Split temporal** (treino/validação/teste) para evitar vazamento.  
- **Tratamento categórico e numérico** via `ColumnTransformer` (imputação + `OneHotEncoder`/escala).  
- **`random_state` fixo** em `train_test_split` e modelos para reprodutibilidade.

> Se já implementado, apenas **comente brevemente** cada etapa (imputação, encoding, escala, balanceamento) e por que ela foi adotada.
"""

# Nota: preferir **split temporal** quando a variável tempo estiver disponível, para evitar vazamento de informação.
# Mantenha `random_state` fixo para reprodutibilidade.

# !pip -q install xgboost shap

import warnings, os, math, itertools, json, textwrap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (precision_score, recall_score, f1_score, roc_auc_score,
                             confusion_matrix, roc_curve, precision_recall_curve, classification_report)
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance

RANDOM_STATE = 42
pd.set_option("display.max_colwidth", 120)

"""## 1) Carregamento do dataset

## EDA e Qualidade do Dado (guia rápido)

Nesta seção, destacamos verificações essenciais para a rubrica:
- **Completude**: valores ausentes/duplicados e decisão de tratamento (imputação/remoção).  
- **Consistência temporal**: verificar ranges e granularidade de datas/horas.  
- **Distribuições**: tipo de crime, local, dia/horário; checar **desbalanceamento** do target.  
- **Outliers**: investigar registros atípicos que possam distorcer o modelo.

> **Nota:** Documente, em Markdown, **o que foi limpo e por quê**, para rastreabilidade e justificativa técnica.
"""

CSV_PATH = "dataset_ocorrencias_delegacia_5.csv"

assert os.path.exists(CSV_PATH), f"Arquivo não encontrado em {CSV_PATH}. Faça upload e ajuste o caminho."
df = pd.read_csv(CSV_PATH)
print(df.shape)
df.head()

"""
## 2) Data storytelling (EDA) e engenharia de atributos de tempo
Aqui criamos atributos derivados de `data_ocorrencia` e uma visão geral dos dados.
"""

df['ts'] = pd.to_datetime(df['data_ocorrencia'])
df['ano'] = df['ts'].dt.year
df['mes'] = df['ts'].dt.to_period('M').astype(str)
df['hora'] = df['ts'].dt.hour
df['dia_semana'] = df['ts'].dt.dayofweek  # 0=segunda ... 6=domingo
df['nome_dia'] = df['ts'].dt.day_name()
df['fim_semana'] = df['dia_semana'].isin([5,6]).astype(int)

# Alvo supervisionado: risco-alto se uso de arma de fogo OU explosivos
df['risco_alto'] = df['arma_utilizada'].isin(['Arma de Fogo', 'Explosivos']).astype(int)

print("Intervalo temporal:", df['ts'].min(), "→", df['ts'].max())
print("Proporção de risco_alto:", df['risco_alto'].mean().round(3))

# Checagem de campos
df.describe(include='all').transpose().head(20)

"""
### 2.1 Gráficos exploratórios
"""

# Distribuição por tipo de crime e taxa de risco_alto
taxa_por_crime = df.groupby('tipo_crime')['risco_alto'].mean().sort_values(ascending=False)
plt.figure()
taxa_por_crime.plot(kind='bar')
plt.title("Taxa de risco alto por tipo de crime")
plt.ylabel("Proporção de risco alto")
plt.xlabel("Tipo de crime")
plt.tight_layout()
plt.show()

# Risco por bairro
taxa_por_bairro = df.groupby('bairro')['risco_alto'].mean().sort_values(ascending=False)
plt.figure()
taxa_por_bairro.plot(kind='bar')
plt.title("Taxa de risco alto por bairro")
plt.ylabel("Proporção de risco alto")
plt.xlabel("Bairro")
plt.tight_layout()
plt.show()

# Série temporal: proporção mensal de risco
taxa_mensal = df.groupby('mes')['risco_alto'].mean()
plt.figure()
taxa_mensal.plot()
plt.title("Proporção de risco alto por mês")
plt.ylabel("Proporção de risco alto")
plt.xlabel("Ano-Mês")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Mapa simples (dispersão) das ocorrências: alto risco vs baixo risco
plt.figure()
plt.scatter(df['longitude'], df['latitude'], s=8, alpha=0.5, label='Todas ocorrências')
plt.title("Dispersão geográfica das ocorrências (todas)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.tight_layout()
plt.show()

plt.figure()
high = df[df['risco_alto']==1]
low  = df[df['risco_alto']==0].sample(min(2000, len(df[df['risco_alto']==0])), random_state=RANDOM_STATE)
plt.scatter(low['longitude'], low['latitude'], s=6, alpha=0.4, label='Baixo risco')
plt.scatter(high['longitude'], high['latitude'], s=8, alpha=0.7, label='Alto risco')
plt.title("Dispersão geográfica: alto vs baixo risco")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.tight_layout()
plt.show()

"""
## 3) Split **temporal** (treino → validação → teste)
Para evitar vazamento, usamos janelas de tempo ordenadas:
- **Treino**: até `2024-06-30`  
- **Validação**: `2024-07-01` a `2025-02-28`  
- **Teste**: `>= 2025-03-01`

"""

cut_train = pd.Timestamp('2024-06-30')
cut_valid = pd.Timestamp('2025-02-28')

train = df[df['ts'] <= cut_train].copy()
valid = df[(df['ts'] > cut_train) & (df['ts'] <= cut_valid)].copy()
test  = df[df['ts'] > cut_valid].copy()

for name, part in [('train',train),('valid',valid),('test',test)]:
    print(name, part['ts'].min(), "→", part['ts'].max(), "| tamanho:", len(part))

assert set(train['id_ocorrencia']).isdisjoint(valid['id_ocorrencia'])
assert set(train['id_ocorrencia']).isdisjoint(test['id_ocorrencia'])
assert set(valid['id_ocorrencia']).isdisjoint(test['id_ocorrencia'])

"""
## 4) Pré-processamento e *feature engineering*
- **Numericas**: `quantidade_vitimas`, `quantidade_suspeitos`, `idade_suspeito`, `dia_semana`  
- **Categóricas**: `bairro`, `tipo_crime`, `sexo_suspeito`, `orgao_responsavel`, `status_investigacao`, `descricao_modus_operandi`  
- **Não usamos**: `arma_utilizada` (é o próprio proxy do alvo), `latitude/longitude` (mantidos para mapas/EDA).


"""

target = 'risco_alto'
num_cols = ['quantidade_vitimas','quantidade_suspeitos','idade_suspeito','dia_semana']
cat_cols = ['bairro','tipo_crime','sexo_suspeito','orgao_responsavel','status_investigacao','descricao_modus_operandi']

X_train, y_train = train[num_cols + cat_cols], train[target]
X_valid, y_valid = valid[num_cols + cat_cols], valid[target]
X_test , y_test  = test [num_cols + cat_cols], test [target]

num_pipe = Pipeline([('scaler', StandardScaler())])
cat_pipe = Pipeline([('oh', OneHotEncoder(handle_unknown='ignore'))])

prep = ColumnTransformer([
    ('num', num_pipe, num_cols),
    ('cat', cat_pipe, cat_cols),
])

# Baselines
dummy = Pipeline([('prep', prep), ('clf', DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE))])
rule_threshold = 0.5  # usado apenas para logging

def rule_based_predict(df_part):
    violent = df_part['tipo_crime'].isin(['Latrocínio','Homicídio','Roubo','Tráfico de Drogas','Estupro','Sequestro'])
    many_suspects = df_part['quantidade_suspeitos'] >= 2
    night_like = df_part['dia_semana'].isin([0,1,2,4,5,6])  # exemplo: segunda, terça, quarta, sexta, sábado, domingo
    # regra simples OR
    return (violent | many_suspects | night_like).astype(int)

"""
## 5) Modelagem: baseline + 2 modelos (LogReg, RandomForest) *(XGBoost opcional)*
"""

def eval_metrics(y_true, y_prob, threshold=0.5, prefix=""):
    y_hat = (y_prob >= threshold).astype(int)
    res = {
        f'{prefix}precision': precision_score(y_true, y_hat, zero_division=0),
        f'{prefix}recall': recall_score(y_true, y_hat, zero_division=0),
        f'{prefix}f1': f1_score(y_true, y_hat, zero_division=0),
        f'{prefix}f1_macro': f1_score(y_true, y_hat, average='macro', zero_division=0),
        f'{prefix}roc_auc': roc_auc_score(y_true, y_prob),
        f'{prefix}support_pos': int(y_true.sum()),
        f'{prefix}support_total': int(len(y_true)),
    }
    return res

# 5.1 Baselines
dummy.fit(X_train, y_train)
prob_valid_dummy = dummy.predict_proba(X_valid)[:,1]

prob_valid_rule  = rule_based_predict(valid)
prob_train_rule  = rule_based_predict(train)

print("Baseline Dummy (valid):", eval_metrics(y_valid, prob_valid_dummy, 0.5, prefix="valid_"))
print("Baseline Regra (valid):", eval_metrics(y_valid, prob_valid_rule.values, 0.5, prefix="valid_"))

# 5.2 Logistic Regression
logreg = Pipeline([('prep', prep),
                   ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))])
logreg.fit(X_train, y_train)
prob_valid_lr = logreg.predict_proba(X_valid)[:,1]

# 5.3 Random Forest
rf = Pipeline([('prep', prep),
               ('clf', RandomForestClassifier(n_estimators=400, max_depth=None, n_jobs=-1, random_state=RANDOM_STATE, class_weight='balanced'))])
rf.fit(X_train, y_train)
prob_valid_rf = rf.predict_proba(X_valid)[:,1]

print("LogReg (valid):", eval_metrics(y_valid, prob_valid_lr, 0.5, prefix="valid_"))
print("RandomForest (valid):", eval_metrics(y_valid, prob_valid_rf, 0.5, prefix="valid_"))

"""
### 5.4 Seleção de limiar (threshold) por **F1** na validação
"""

def best_threshold_by_f1(y_true, y_prob):
    thresholds = np.linspace(0.2, 0.8, 61)
    f1s = [f1_score(y_true, (y_prob>=t).astype(int), zero_division=0) for t in thresholds]
    best_i = int(np.argmax(f1s))
    return float(thresholds[best_i]), float(f1s[best_i])

thr_lr, best_f1_lr = best_threshold_by_f1(y_valid, prob_valid_lr)
thr_rf, best_f1_rf = best_threshold_by_f1(y_valid, prob_valid_rf)

print("Melhor threshold LogReg:", thr_lr, "F1_valid:", best_f1_lr)
print("Melhor threshold RandomForest:", thr_rf, "F1_valid:", best_f1_rf)

best_model_name = "rf" if best_f1_rf >= best_f1_lr else "lr"
best_model = rf if best_model_name == "rf" else logreg
best_thr = thr_rf if best_model_name == "rf" else thr_lr
print("Modelo escolhido:", best_model_name, "| threshold:", best_thr)

"""
## 6) Avaliação final no **teste**
"""

prob_test = best_model.predict_proba(X_test)[:,1]
metrics_test = eval_metrics(y_test, prob_test, threshold=best_thr, prefix="test_")
metrics_test

# Matriz de confusão, Curvas ROC e PR
y_hat_test = (prob_test >= best_thr).astype(int)
cm = confusion_matrix(y_test, y_hat_test)

plt.figure()
plt.imshow(cm, cmap='Blues')
plt.title("Matriz de Confusão (Teste)")
plt.colorbar()
plt.xticks([0,1], ['Pred 0','Pred 1'])
plt.yticks([0,1], ['True 0','True 1'])
for (i, j), z in np.ndenumerate(cm):
    plt.text(j, i, str(z), ha='center', va='center')
plt.tight_layout()
plt.show()

fpr, tpr, _ = roc_curve(y_test, prob_test)
plt.figure()
plt.plot(fpr, tpr)
plt.plot([0,1],[0,1],'--')
plt.title("ROC curve (Teste)")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.tight_layout()
plt.show()

prec, rec, _ = precision_recall_curve(y_test, prob_test)
plt.figure()
plt.plot(rec, prec)
plt.title("Precision-Recall (Teste)")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.tight_layout()
plt.show()

"""
## 7) Interpretação: importância de features (*Permutation Importance*)
Executada no conjunto de **validação**, sobre o **modelo escolhido**.
"""

# === Permutation Importance (fix: converter X para denso) ===
def get_prep_and_model(fitted_pipeline):
    prep = fitted_pipeline.named_steps['prep']
    model = fitted_pipeline.named_steps['clf']
    return prep, model

prep_fitted, clf_fitted = get_prep_and_model(best_model)
X_valid_trans = prep_fitted.transform(X_valid)

# >>> ADICIONE ESTAS 2 LINHAS <<<
if hasattr(X_valid_trans, "toarray"):
    X_valid_trans = X_valid_trans.toarray()

# Montar nomes das features transformadas
feat_names = []
feat_names += num_cols
oh = prep_fitted.named_transformers_['cat'].named_steps['oh']
oh_names = oh.get_feature_names_out(cat_cols).tolist()
feat_names += oh_names

from sklearn.inspection import permutation_importance
imp = permutation_importance(
    clf_fitted,
    X_valid_trans,
    y_valid,
    n_repeats=10,
    random_state=RANDOM_STATE,
    scoring='f1'
)
sorted_idx = np.argsort(imp.importances_mean)[::-1][:20]

plt.figure()
plt.bar(range(len(sorted_idx)), imp.importances_mean[sorted_idx])
plt.xticks(range(len(sorted_idx)), [feat_names[i] for i in sorted_idx], rotation=90)
plt.title("Top 20 - Permutation Importance (validação)")
plt.tight_layout()
plt.show()

"""
## 8) Métrica operacional (**F1@k** por *bairro/mês*)
**Ideia**: Em cada janela (bairro, mês), priorizar **K%** das ocorrências com maior risco previsto.  
Comparamos com a verdade (`risco_alto`) para obter **Precision/Recall/F1** e, assim, estimar **quão útil** é a priorização de patrulhamento/apoio.

> Por padrão, usamos `k_pct = 0.20` (top 20%).
"""

def f1_at_k_by_window(df_part, probs, k_pct=0.20, window_cols=('bairro','mes'), thr=None):
    d = df_part.copy()
    d = d.assign(prob=probs, y=d['risco_alto'].astype(int))
    keys = list(window_cols)
    frames = []
    for key, g in d.groupby(keys):
        if len(g) == 0:
            continue
        k = max(1, int(len(g) * k_pct))
        g_sorted = g.sort_values('prob', ascending=False)
        picked = g_sorted.head(k).copy()
        # marca top-k como positivos previstos
        picked['y_hat'] = 1
        rest = g_sorted.iloc[k:].copy()
        rest['y_hat'] = 0
        gg = pd.concat([picked, rest], ignore_index=True)
        p = precision_score(gg['y'], gg['y_hat'], zero_division=0)
        r = recall_score(gg['y'], gg['y_hat'], zero_division=0)
        f = f1_score(gg['y'], gg['y_hat'], zero_division=0)
        frames.append(pd.DataFrame([{'window': key, 'n': len(g), 'precision': p, 'recall': r, 'f1': f}]))
    out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=['window','n','precision','recall','f1'])
    return out

f1k = f1_at_k_by_window(test, prob_test, k_pct=0.20, window_cols=('bairro','mes'))
print("Média F1@20% (bairro/mês):", f1k['f1'].mean().round(3))
f1k.sort_values('f1', ascending=False).head(10)

# Visualização simples do F1@k médio por bairro (agregado no teste)
f1k_bairro = f1k.groupby(f1k['window'].apply(lambda x: x[0]))['f1'].mean().sort_values(ascending=False)
plt.figure()
f1k_bairro.plot(kind='bar')
plt.title("F1@20% por bairro (médio, teste)")
plt.ylabel("F1")
plt.xlabel("Bairro")
plt.tight_layout()
plt.show()

"""
## 9) Tabelas de risco para **recomendações operacionais**
Vamos sumarizar combinações com **alto risco previsto** para orientar protocolos.
"""

test_with_prob = test.copy()
test_with_prob['prob_risco'] = prob_test
# Top 20% mais arriscados
thr_top20 = np.quantile(test_with_prob['prob_risco'], 0.80)
top_danger = test_with_prob[test_with_prob['prob_risco'] >= thr_top20]

# Top combinações
tbl1 = (top_danger.groupby(['bairro','tipo_crime'])
        .size().reset_index(name='qtd')
        .sort_values('qtd', ascending=False).head(12))

tbl2 = (top_danger.groupby(['tipo_crime','descricao_modus_operandi'])
        .size().reset_index(name='qtd')
        .sort_values('qtd', ascending=False).head(12))

tbl1, tbl2

"""## 10) Conclusões (resumo no notebook)
- O alvo `risco_alto` (armamento perigoso) permite priorização tática **antes** do deslocamento da equipe.
- O **modelo escolhido** (pela validação) e o **threshold** selecionado maximizam **F1** (equilíbrio entre Precision/Recall).
- **Bairros** e **tipos de crime** específicos concentram maior probabilidade de risco, o que sustenta rotas e protocolos reforçados.
- A métrica **F1@k** mostra o **ganho operacional** ao priorizar o *top-20%* de chamados por (bairro, mês).
- **Importância de features** indica os fatores mais fortes para risco; isso direciona **treinamento** e **cheklist** nas centrais.
- Limitações: ausência de rótulos "acidente com policial", viés de registro, granularidade de hora/turno; próximos passos no final do relatório textual.

## Visualização e Apresentação

Para aumentar a clareza do notebook na avaliação:
- Títulos e legendas em todos os gráficos.  
- Eixos e unidades padronizados.  
- Figuras-chave exportadas para `/reports` se exigido pelo professor.

# Análise Não Supervisionada (Clusterização)

Nesta seção atendemos ao requisito de **não supervisionado** da Entrega 2.
Objetivo: **agrupar ocorrências** por sinais de **local/tempo/modus operandi** para revelar *padrões operacionais*.
- Algoritmo: `KMeans` (baseline de clusterização). Opcional: testar `DBSCAN/HDBSCAN`.
- Validação: **Silhouette Score** e interpretação dos clusters.
- Saídas esperadas: caracterização de clusters (turno, tipo, região) e implicações para risco operacional.
"""

# Pré-requisito: garantir um DataFrame `df_model` com colunas já pré-processadas para clusterização.
# Sugestão: usar variáveis como bairro/região (one-hot), hora do dia (cíclica), dia da semana, tipo_crime (one-hot).
# Atenção à LGPD: nenhuma coluna com PII deve entrar aqui.

import numpy as np
import pandas as pd

# Exemplo de seleção de features (ajuste para suas colunas reais)
# cols_cluster = ['hora_sin','hora_cos','dia_semana','regiao_X','regiao_Y','tipo_assalto','tipo_roubo']  # EXEMPLO
# X_cluster = df_model[cols_cluster].copy()

# Placeholder seguro (substitua quando tiver `df_model` pronto):
try:
    X_cluster  # noqa: F821
except NameError:
    X_cluster = None
    print("[AVISO] Defina `X_cluster` com as colunas numéricas/codificadas para clusterização.")

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Defina um range de k e escolha via Silhouette
def kmeans_silhouette_search(X, ks=(2,3,4,5,6), random_state=42):
    results = []
    for k in ks:
        km = KMeans(n_clusters=k, n_init=10, random_state=random_state)
        labels = km.fit_predict(X)
        sil = silhouette_score(X, labels)
        results.append((k, sil))
    return results

if X_cluster is not None:
    ks = range(2,8)
    sil_results = kmeans_silhouette_search(X_cluster, ks=ks, random_state=42)
    print("Silhouette por k:", sil_results)

    # Seleciona o melhor k
    best_k = max(sil_results, key=lambda t: t[1])[0]
    kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=42)
    cluster_labels = kmeans.fit_predict(X_cluster)
    # Anexa rótulos para interpretação posterior
    # df_model['cluster_kmeans'] = cluster_labels  # descomente quando df_model existir
else:
    print("[INFO] Pule esta célula até definir `X_cluster`.")

"""### Interpretação dos clusters
- Descrever brevemente **o que caracteriza cada cluster** (ex.: *Cluster 0 = crimes violentos à noite em bairros X/Y*).
- Relacionar clusters com **risco ao policial** (alta frequência de tipos perigosos; concentração espacial/temporal).

"""

# Exemplo de sumarização por cluster (ajuste nomes de colunas):
# if X_cluster is not None and 'cluster_kmeans' in df_model.columns:
#     resumo = (df_model
#               .join(pd.Series(cluster_labels, name='cluster_kmeans'), how='left')
#               .groupby('cluster_kmeans')
#               .agg(freq=('cluster_kmeans','size'),
#                    hora_media=('hora_evento','mean'),
#                    pct_tipo_perigoso=('crime_perigoso','mean'))
#               .sort_values('freq', ascending=False))
#     display(resumo.head(10))

"""# Melhoria do Modelo Supervisionado (Tuning + Comparação)

Atende ao critério de **aprimoramento do supervisionado** com `GridSearchCV`/`RandomizedSearchCV` e comparação de métricas **antes/depois**.

"""

# Exemplo com RandomForestClassifier (ajuste variáveis X_train, y_train, X_val, y_val)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Parâmetros exemplo (reduzidos para agilidade)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

# rf_base = RandomForestClassifier(random_state=42, n_estimators=100)  # modelo que vocês já tinham
# rf_base.fit(X_train, y_train)
# y_pred_base = rf_base.predict(X_val)
# print("[BASE] Métricas (val):\n", classification_report(y_val, y_pred_base, zero_division=0))

# rf_gs = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=param_grid,
#                      scoring='f1_macro', cv=3, n_jobs=-1, verbose=1)
# rf_gs.fit(X_train, y_train)
# print("Melhor combinação:", rf_gs.best_params_)
# y_pred_gs = rf_gs.predict(X_val)
# print("[TUNED] Métricas (val):\n", classification_report(y_val, y_pred_gs, zero_division=0))

# Observação: replicar a comparação para o modelo 2 (ex.: XGBClassifier) se aplicável.

"""### Drift temporal e fairness (opcional com pontos extras)
- **Drift**: comparar métricas entre janelas temporais (ex.: 2023 vs 2024).
- **Fairness operacional**: comparar erros por **região** ou **turno**.
Registre qualquer **viés operacional** relevante e impacto para a tomada de decisão.

# Dashboard Analítico (no notebook)

Sem usar Streamlit/Power BI, esta seção resume os principais **achados operacionais** em formato de *mini-dashboard*:
- **Top 10 locais/áreas de maior risco** (tabela).
- **Matriz de confusão** do melhor modelo.
- **Importância de features** (Permutation Importance/árvore).
- **Distribuição temporal** (por hora/dia) dos eventos perigosos.
- **Resumo textual** em bullets com recomendações operacionais.
"""

# Exemplos de esqueleto para as visualizações (ajuste conforme suas variáveis e objetos já treinados):
# from sklearn.metrics import ConfusionMatrixDisplay
# import matplotlib.pyplot as plt
# fig, ax = plt.subplots()
# ConfusionMatrixDisplay.from_estimator(rf_gs.best_estimator_, X_val, y_val, ax=ax)  # ou modelo escolhido
# ax.set_title("Matriz de Confusão - Melhor Modelo")
# plt.show()

# # Importância de features (árvores) — se seu pipeline já expõe as features finais:
# import numpy as np
# feat_names = np.array(feature_names_final)  # defina a lista de nomes após o ColumnTransformer
# importances = rf_gs.best_estimator_.feature_importances_
# top_idx = np.argsort(importances)[::-1][:15]
# display(pd.DataFrame({"feature": feat_names[top_idx], "importance": importances[top_idx]}))

# # Ranking de áreas mais perigosas (exemplo):
# top_areas = (df_model.groupby('area')
#                      .agg(risco=('crime_perigoso','mean'), qtd=('area','size'))
#                      .sort_values(['risco','qtd'], ascending=False)
#                      .head(10))
# display(top_areas)

"""## Conclusões Operacionais
- **Áreas críticas**: …  
- **Janelas temporais de maior risco**: …  
- **Tipos de crime com maior impacto no risco**: …  
- **Recomendações**: … (ex.: reforçar patrulhamento noturno em áreas/turnos X).

# LGPD, Ética, Reprodutibilidade e Documentação (Consolidado)

**Resumo:** Esta seção reúne todas as notas de LGPD/ética e reprodutibilidade em um único bloco, conforme solicitado pelo professor.

## LGPD e Ética
- Não utilizamos dados pessoais identificáveis (PII). Em caso de potencial de reidentificação, aplicar **anonimização**/**pseudonimização** e reduzir granularidade espaço-temporal.
- Uso acadêmico e de interesse público, seguindo princípios de **finalidade**, **necessidade** e **minimização**.
- Evitar exposição de campos sensíveis em figuras e tabelas.

## Reprodutibilidade
- `random_state`/`seed` fixos em splits e modelos.
- Dependências listadas em `requirements.txt` (ou `environment.yml`). Executar em ambiente isolado.
- Passos de execução: (1) preparar/baixar dados; (2) executar pipeline; (3) rodar clusterização; (4) treinar/tunar modelos; (5) gerar métricas/figuras/dashboard; (6) registrar achados.

## Documentação
- Estrutura de repo: `/data`, `/notebooks`, `/src`, `/reports`, `README.md`, `requirements.txt`.
- Descrever o **alvo** (`risco_alto`), as **features** e as **métricas** usadas (F1, precision, recall, ROC/PR).

_Data desta versão:_ 2025-10-04

# App Streamlit – Painel Interativo de Risco Policial

Este esqueleto cria um **painel interativo** com os **dados reais** do notebook:
- Filtros por **bairro** e **tipo de crime**.
- **Métricas** do modelo supervisionado (se disponíveis).
- **Ranking** de áreas mais arriscadas.
- **Mapa** das ocorrências (Folium).
- **Distribuições temporais**.

> Para executar como app local, salve o bloco seguinte em um arquivo `app.py` e rode: `streamlit run app.py`.
"""

# ==== App Streamlit (usar dados reais do notebook) ====
# Observação: Este código pressupõe que você já tem no ambiente:
# - `df` com colunas: 'bairro', 'tipo_crime', 'risco_alto', 'latitude', 'longitude'
# - `prob_risco` associado ao conjunto de teste (opcional para ranking)
# - dicionário `metrics_test` (opcional) com chaves como 'test_f1', 'test_precision', 'test_recall'

import pandas as pd
import numpy as np

try:
    import streamlit as st
except Exception as e:
    print("[Aviso] Instale o Streamlit para executar o app: pip install streamlit streamlit-folium folium")

# Imports opcionais para mapa
try:
    import folium
    from streamlit_folium import st_folium
except Exception as e:
    folium = None

# ===== Helpers =====
def get_df():
    # Usa df já carregado no notebook se existir
    global df
    if isinstance(df, pd.DataFrame):
        return df.copy()
    # Como fallback, tente CSV_PATH definido no notebook
    try:
        from __main__ import CSV_PATH
        if CSV_PATH:
            return pd.read_csv(CSV_PATH)
    except Exception:
        pass
    raise RuntimeError("Defina `df` no notebook ou ajuste a leitura de dados.")

def safe_columns(d, cols):
    missing = [c for c in cols if c not in d.columns]
    if missing:
        raise RuntimeError(f"Colunas ausentes no DataFrame: {missing}")
    return d[cols]

# ===== App =====
def run_app():
    st.set_page_config(page_title="Risco Policial – MVP", layout="wide")
    st.title("Painel de Risco Policial (MVP)")

    d = get_df()
    # Garantir colunas essenciais
    base_cols = ['bairro', 'tipo_crime', 'risco_alto']
    d = safe_columns(d, base_cols + [c for c in ['latitude','longitude'] if c in d.columns])

    # Filtros
    bairros = ["Todos"] + sorted([x for x in d['bairro'].dropna().unique().tolist()])
    tipos = ["Todos"] + sorted([x for x in d['tipo_crime'].dropna().unique().tolist()])

    col1, col2 = st.columns(2)
    with col1:
        b_sel = st.selectbox("Bairro", bairros, index=0)
    with col2:
        t_sel = st.selectbox("Tipo de crime", tipos, index=0)

    df_view = d.copy()
    if b_sel != "Todos":
        df_view = df_view[df_view['bairro'] == b_sel]
    if t_sel != "Todos":
        df_view = df_view[df_view['tipo_crime'] == t_sel]

    st.subheader("Visão geral dos filtros")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("Ocorrências", len(df_view))
    with c2:
        st.metric("Proporção de alto risco", f"{df_view['risco_alto'].mean():.1%}" if len(df_view)>0 else "N/A")
    with c3:
        if 'metrics_test' in globals():
            mt = globals()['metrics_test']
            f1 = mt.get('test_f1', None)
            prec = mt.get('test_precision', None)
            rec = mt.get('test_recall', None)
            st.metric("F1 (teste)", f"{float(f1):.3f}" if f1 is not None else "—")
            st.caption(f"Precision: {prec:.3f} | Recall: {rec:.3f}" if prec is not None and rec is not None else "")
        else:
            st.write("Métricas (teste) não encontradas no ambiente.")

    # Ranking de risco por bairro (usando prob_risco se existir)
    st.subheader("Top bairros por risco (estimado)")
    if 'prob_risco' in globals() and isinstance(globals()['prob_risco'], (list, np.ndarray, pd.Series)):
        # Tentativa de montar tabela unindo `test` e `prob_risco`. O notebook mostra um exemplo com 'test_with_prob'.
        if 'test_with_prob' in globals():
            twp = globals()['test_with_prob'].copy()
            if 'bairro' in twp.columns and 'prob_risco' in twp.columns:
                rank = (twp.groupby('bairro')['prob_risco']
                        .mean().sort_values(ascending=False).head(10).reset_index())
                st.dataframe(rank)
            else:
                st.write("`test_with_prob` não contém as colunas esperadas.")
        else:
            # Fallback: usar df_view
            rank = (df_view.groupby('bairro')['risco_alto']
                    .mean().sort_values(ascending=False).head(10).reset_index(name='risco_medio'))
            st.dataframe(rank)
    else:
        rank = (df_view.groupby('bairro')['risco_alto']
                .mean().sort_values(ascending=False).head(10).reset_index(name='risco_medio'))
        st.dataframe(rank)

    # Mapa (se latitude/longitude disponíveis e folium instalado)
    if folium is not None and {'latitude','longitude'}.issubset(df_view.columns):
        st.subheader("Mapa de ocorrências")
        lat0 = float(df_view['latitude'].mean())
        lon0 = float(df_view['longitude'].mean())
        m = folium.Map(location=[lat0, lon0], zoom_start=12)
        sample = df_view.sample(min(1000, len(df_view)), random_state=42) if len(df_view)>0 else df_view
        for _, r in sample.iterrows():
            color = 'red' if r.get('risco_alto',0)==1 else 'blue'
            folium.CircleMarker(
                location=[r['latitude'], r['longitude']],
                radius=3, color=color, fill=True, fill_opacity=0.6
            ).add_to(m)
        st_folium(m, width=900, height=500)
    else:
        st.info("Mapa indisponível (bibliotecas não instaladas ou colunas latitude/longitude ausentes).")

    # Distribuição temporal (se colunas existirem)
    for time_col in ['mes', 'dia_semana']:
        if time_col in df_view.columns:
            st.subheader(f"Distribuição por {time_col}")
            tb = (df_view.groupby(time_col)['risco_alto']
                  .mean().reset_index(name='risco_medio'))
            st.bar_chart(tb.set_index(time_col))

# Para rodar dentro do Streamlit:
# if __name__ == "__main__":
#     run_app()